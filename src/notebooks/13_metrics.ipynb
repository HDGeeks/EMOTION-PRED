{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac65636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Project root: /Users/hd/Desktop/EMOTION-PRED\n",
      "ðŸ“‚ Source root: /Users/hd/Desktop/EMOTION-PRED/src\n",
      "ðŸ“‚ Results root: /Users/hd/Desktop/EMOTION-PRED/src/results\n",
      "ðŸ“‚ Data root: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n",
      "Using dataset directory: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Running as normal Python script inside src/\n",
    "    this_file = os.path.abspath(__file__)\n",
    "    src_root = os.path.dirname(this_file)                        # EMOTION-PRED/src\n",
    "    project_root = os.path.dirname(src_root)                    # EMOTION-PRED/\n",
    "except NameError:\n",
    "    # Running inside Jupyter (likely src/notebooks or src/)\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # If running inside src/notebooks â†’ go up one level\n",
    "    if cwd.endswith(\"notebooks\"):\n",
    "        src_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "        project_root = os.path.dirname(src_root)\n",
    "    else:\n",
    "        # Running from project root directly\n",
    "        project_root = cwd\n",
    "        src_root = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Final unified paths\n",
    "results_root = os.path.join(src_root, \"results\")\n",
    "data_root = os.path.join(src_root, \"data\",\"MAMS-ACSA\",\"raw\",\"data_jsonl\")\n",
    "print(f\"ðŸ“‚ Project root: {project_root}\"\n",
    "      f\"\\nðŸ“‚ Source root: {src_root}\"\n",
    "      f\"\\nðŸ“‚ Results root: {results_root}\"\n",
    "      f\"\\nðŸ“‚ Data root: {data_root}\")\n",
    "# 3 â€” JSONL files\n",
    "TRAIN_JSONL = os.path.join(data_root, \"train.jsonl\")\n",
    "VAL_JSONL   = os.path.join(data_root, \"val.jsonl\")\n",
    "TEST_JSONL  = os.path.join(data_root, \"test.jsonl\")\n",
    "SAMPLE_JSONL = os.path.join(data_root, \"sample.jsonl\")\n",
    "print(\"Using dataset directory:\", data_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8741d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # VALIDATION SCRIPT FOR 300 GOLD ANNOTATIONS\n",
    "# # Evaluates:\n",
    "# #  - Emotion Macro-F1 / Micro-F1\n",
    "# #  - Aspect Macro-F1\n",
    "# #  - Polarity Macro-F1\n",
    "# #  - Full ABSA triple accuracy\n",
    "# # ============================================================\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # PATHS - UPDATE ONLY THESE IF NEEDED\n",
    "# # ------------------------------------------------------------\n",
    "# GOLD_PATH = os.path.join(data_root, \"sample_06_12_2025_6pm_annotated.jsonl\")\n",
    "\n",
    "# # Option A â†’ Emotion-only model output\n",
    "# PRED_PATH = \"output/gemini_annotated.jsonl\"\n",
    "\n",
    "# # Option B â†’ Full aspect+polarity+emotion prediction\n",
    "# # PRED_PATH = \"output/gemini_full_absa.jsonl\"\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # LOAD FILES\n",
    "# # ------------------------------------------------------------\n",
    "# def load_jsonl(path):\n",
    "#     rows = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             rows.append(json.loads(line))\n",
    "#     return rows\n",
    "\n",
    "# gold = load_jsonl(GOLD_PATH)\n",
    "# pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# assert len(gold) == len(pred), f\"Row mismatch: gold={len(gold)}, pred={len(pred)}\"\n",
    "# print(f\"Loaded {len(gold)} rows âœ“\")\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # EXPLODE INTO FLAT FORM\n",
    "# # ------------------------------------------------------------\n",
    "# def explode(data):\n",
    "#     rows = []\n",
    "#     for i, row in enumerate(data):\n",
    "#         for item in row[\"output\"]:\n",
    "#             rows.append({\n",
    "#                 \"id\": i,\n",
    "#                 \"aspect\": item.get(\"aspect\"),\n",
    "#                 \"polarity\": item.get(\"polarity\"),\n",
    "#                 \"emotion\": item.get(\"emotion\")\n",
    "#             })\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "# gold_df = explode(gold)\n",
    "# pred_df = explode(pred)\n",
    "\n",
    "# print(\"Gold rows:\", len(gold_df))\n",
    "# print(\"Pred rows:\", len(pred_df))\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # MERGE GOLD + PRED ON (ID, ASPECT, POLARITY)\n",
    "# # ------------------------------------------------------------\n",
    "# merged = gold_df.merge(\n",
    "#     pred_df,\n",
    "#     on=[\"id\", \"aspect\", \"polarity\"],\n",
    "#     suffixes=(\"_gold\", \"_pred\"),\n",
    "#     how=\"outer\"\n",
    "# )\n",
    "\n",
    "# print(\"Merged rows:\", len(merged))\n",
    "\n",
    "# # Drop rows where emotion missing in gold or pred\n",
    "# valid = merged.dropna(subset=[\"emotion_gold\", \"emotion_pred\"])\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # EMOTION F1\n",
    "# # ------------------------------------------------------------\n",
    "# y_true = valid[\"emotion_gold\"]\n",
    "# y_pred = valid[\"emotion_pred\"]\n",
    "\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"EMOTION SCORES\")\n",
    "# print(\"==============================\")\n",
    "# print(\"Emotion Macro-F1:\", f1_score(y_true, y_pred, average='macro'))\n",
    "# print(\"Emotion Micro-F1:\", f1_score(y_true, y_pred, average='micro'))\n",
    "\n",
    "# print(\"\\nClassification report:\")\n",
    "# print(classification_report(y_true, y_pred))\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # ASPECT F1 (if full ABSA predictions contain aspects)\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"ASPECT SCORES\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# aspect_true = gold_df[\"aspect\"]\n",
    "# aspect_pred = pred_df[\"aspect\"]\n",
    "\n",
    "# aspect_f1 = f1_score(aspect_true, aspect_pred, average=\"macro\")\n",
    "# print(\"Aspect Macro-F1:\", aspect_f1)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # POLARITY F1\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"POLARITY SCORES\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# pol_true = gold_df[\"polarity\"]\n",
    "# pol_pred = pred_df[\"polarity\"]\n",
    "\n",
    "# pol_f1 = f1_score(pol_true, pol_pred, average=\"macro\")\n",
    "# print(\"Polarity Macro-F1:\", pol_f1)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FULL ABSA TRIPLE ACCURACY\n",
    "# # aspect + polarity + emotion must match exactly\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"FULL ABSA TRIPLE ACCURACY\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# merged[\"triple_gold\"] = list(\n",
    "#     zip(merged[\"aspect\"], merged[\"polarity\"], merged[\"emotion_gold\"])\n",
    "# )\n",
    "# merged[\"triple_pred\"] = list(\n",
    "#     zip(merged[\"aspect\"], merged[\"polarity\"], merged[\"emotion_pred\"])\n",
    "# )\n",
    "\n",
    "# merged[\"correct_triple\"] = merged[\"triple_gold\"] == merged[\"triple_pred\"]\n",
    "# triple_accuracy = merged[\"correct_triple\"].mean()\n",
    "\n",
    "# print(\"Full ABSA Accuracy:\", triple_accuracy)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # SUMMARY TABLE\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"SUMMARY\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# print(f\"\"\"\n",
    "# Emotion Macro-F1      : {f1_score(y_true, y_pred, average='macro'):.4f}\n",
    "# Emotion Micro-F1      : {f1_score(y_true, y_pred, average='micro'):.4f}\n",
    "# Aspect Macro-F1       : {aspect_f1:.4f}\n",
    "# Polarity Macro-F1     : {pol_f1:.4f}\n",
    "# Full ABSA Accuracy    : {triple_accuracy:.4f}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45cc69d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'output/gemini_annotated_aspect_polarity_emotions.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 128\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Evaluate ALL prediction files\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    126\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPRED_DIR\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotated\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fname:\n\u001b[1;32m    130\u001b[0m         pred_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PRED_DIR, fname)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'output/gemini_annotated_aspect_polarity_emotions.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "GOLD_PATH = os.path.join(data_root, \"sample_06_12_2025_6pm_annotated.jsonl\")\n",
    "PRED_DIR  = os.path.join(src_root,\"notebooks\",\"output\",\"gemini_annotated_aspect_polarity_emotions.jsonl\")  # folder with Gemini / GPT / LLaMA predictions\n",
    "\n",
    "# -----------------------------\n",
    "# Load JSONL\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    return [json.loads(line) for line in open(path, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "gold = load_jsonl(GOLD_PATH)\n",
    "\n",
    "# Index gold by text for alignment\n",
    "gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "\n",
    "# -----------------------------\n",
    "# Convert triple list into flat lists for scoring\n",
    "# -----------------------------\n",
    "def flatten_triples(triples):\n",
    "    \"\"\"\n",
    "    Each review creates N triples â†’ but classification requires aligned lists.\n",
    "    We treat each triple independently.\n",
    "    \"\"\"\n",
    "    aspects = []\n",
    "    polarities = []\n",
    "    emotions = []\n",
    "    triples_clean = []\n",
    "\n",
    "    for t in triples:\n",
    "        asp = t[\"aspect\"]\n",
    "        pol = t[\"polarity\"]\n",
    "        emo = t[\"emotion\"]\n",
    "\n",
    "        aspects.append(asp)\n",
    "        polarities.append(pol)\n",
    "        emotions.append(emo)\n",
    "        triples_clean.append((asp, pol, emo))\n",
    "\n",
    "    return aspects, polarities, emotions, triples_clean\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate a single prediction file\n",
    "# -----------------------------\n",
    "def evaluate_prediction_file(pred_path):\n",
    "    pred = load_jsonl(pred_path)\n",
    "\n",
    "    # Align using input text\n",
    "    pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "    gold_aspects = []\n",
    "    gold_pols    = []\n",
    "    gold_emots   = []\n",
    "    gold_triples = []\n",
    "\n",
    "    pred_aspects = []\n",
    "    pred_pols    = []\n",
    "    pred_emots   = []\n",
    "    pred_triples = []\n",
    "\n",
    "    # Iterate through all rows\n",
    "    for text, gold_list in gold_map.items():\n",
    "        pred_list = pred_map.get(text, [])\n",
    "\n",
    "        ga, gp, ge, gt = flatten_triples(gold_list)\n",
    "        pa, pp, pe, pt = flatten_triples(pred_list)\n",
    "\n",
    "        gold_aspects.extend(ga)\n",
    "        gold_pols.extend(gp)\n",
    "        gold_emots.extend(ge)\n",
    "        gold_triples.extend(gt)\n",
    "\n",
    "        pred_aspects.extend(pa)\n",
    "        pred_pols.extend(pp)\n",
    "        pred_emots.extend(pe)\n",
    "        pred_triples.extend(pt)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Aspect F1\n",
    "    # -----------------------------\n",
    "    aspect_f1 = f1_score(gold_aspects, pred_aspects, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Polarity F1\n",
    "    # -----------------------------\n",
    "    polarity_f1 = f1_score(gold_pols, pred_pols, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Emotion F1 (macro/micro)\n",
    "    # -----------------------------\n",
    "    emo_macro = f1_score(gold_emots, pred_emots, average=\"macro\", zero_division=0)\n",
    "    emo_micro = f1_score(gold_emots, pred_emots, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # -----------------------------\n",
    "    # FULL ABSA Exact Triple Match\n",
    "    # -----------------------------\n",
    "    gold_counter = Counter(gold_triples)\n",
    "    pred_counter = Counter(pred_triples)\n",
    "\n",
    "    match = sum((gold_counter & pred_counter).values())\n",
    "    total = len(gold_triples)\n",
    "    exact_acc = match / total if total > 0 else 0\n",
    "\n",
    "    # Return structured scores\n",
    "    return {\n",
    "        \"file\": os.path.basename(pred_path),\n",
    "        \"Aspect F1\": aspect_f1,\n",
    "        \"Polarity F1\": polarity_f1,\n",
    "        \"Emotion Macro-F1\": emo_macro,\n",
    "        \"Emotion Micro-F1\": emo_micro,\n",
    "        \"Full ABSA Accuracy\": exact_acc\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate ALL prediction files\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for fname in os.listdir(PRED_DIR):\n",
    "    if fname.endswith(\".jsonl\") and \"annotated\" in fname:\n",
    "        pred_path = os.path.join(PRED_DIR, fname)\n",
    "        print(\"Evaluating:\", pred_path)\n",
    "        scores = evaluate_prediction_file(pred_path)\n",
    "        results.append(scores)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "# Save CSV summary\n",
    "df.to_csv(\"absa_model_comparison.csv\", index=False)\n",
    "print(\"\\nSaved â†’ absa_model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
