{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac65636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Project root: /Users/hd/Desktop/EMOTION-PRED\n",
      "ðŸ“‚ Source root: /Users/hd/Desktop/EMOTION-PRED/src\n",
      "ðŸ“‚ Results root: /Users/hd/Desktop/EMOTION-PRED/src/results\n",
      "ðŸ“‚ Data root: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n",
      "Using dataset directory: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Running as normal Python script inside src/\n",
    "    this_file = os.path.abspath(__file__)\n",
    "    src_root = os.path.dirname(this_file)                        # EMOTION-PRED/src\n",
    "    project_root = os.path.dirname(src_root)                    # EMOTION-PRED/\n",
    "except NameError:\n",
    "    # Running inside Jupyter (likely src/notebooks or src/)\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # If running inside src/notebooks â†’ go up one level\n",
    "    if cwd.endswith(\"notebooks\"):\n",
    "        src_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "        project_root = os.path.dirname(src_root)\n",
    "    else:\n",
    "        # Running from project root directly\n",
    "        project_root = cwd\n",
    "        src_root = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Final unified paths\n",
    "results_root = os.path.join(src_root, \"results\")\n",
    "data_root = os.path.join(src_root, \"data\",\"MAMS-ACSA\",\"raw\",\"data_jsonl\")\n",
    "print(f\"ðŸ“‚ Project root: {project_root}\"\n",
    "      f\"\\nðŸ“‚ Source root: {src_root}\"\n",
    "      f\"\\nðŸ“‚ Results root: {results_root}\"\n",
    "      f\"\\nðŸ“‚ Data root: {data_root}\")\n",
    "# 3 â€” JSONL files\n",
    "TRAIN_JSONL = os.path.join(data_root, \"train.jsonl\")\n",
    "VAL_JSONL   = os.path.join(data_root, \"val.jsonl\")\n",
    "TEST_JSONL  = os.path.join(data_root, \"test.jsonl\")\n",
    "SAMPLE_JSONL = os.path.join(data_root, \"sample.jsonl\")\n",
    "print(\"Using dataset directory:\", data_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8741d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # VALIDATION SCRIPT FOR 300 GOLD ANNOTATIONS\n",
    "# # Evaluates:\n",
    "# #  - Emotion Macro-F1 / Micro-F1\n",
    "# #  - Aspect Macro-F1\n",
    "# #  - Polarity Macro-F1\n",
    "# #  - Full ABSA triple accuracy\n",
    "# # ============================================================\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # PATHS - UPDATE ONLY THESE IF NEEDED\n",
    "# # ------------------------------------------------------------\n",
    "# GOLD_PATH = os.path.join(data_root, \"sample_06_12_2025_6pm_annotated.jsonl\")\n",
    "\n",
    "# # Option A â†’ Emotion-only model output\n",
    "# PRED_PATH = \"output/gemini_annotated.jsonl\"\n",
    "\n",
    "# # Option B â†’ Full aspect+polarity+emotion prediction\n",
    "# # PRED_PATH = \"output/gemini_full_absa.jsonl\"\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # LOAD FILES\n",
    "# # ------------------------------------------------------------\n",
    "# def load_jsonl(path):\n",
    "#     rows = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             rows.append(json.loads(line))\n",
    "#     return rows\n",
    "\n",
    "# gold = load_jsonl(GOLD_PATH)\n",
    "# pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# assert len(gold) == len(pred), f\"Row mismatch: gold={len(gold)}, pred={len(pred)}\"\n",
    "# print(f\"Loaded {len(gold)} rows âœ“\")\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # EXPLODE INTO FLAT FORM\n",
    "# # ------------------------------------------------------------\n",
    "# def explode(data):\n",
    "#     rows = []\n",
    "#     for i, row in enumerate(data):\n",
    "#         for item in row[\"output\"]:\n",
    "#             rows.append({\n",
    "#                 \"id\": i,\n",
    "#                 \"aspect\": item.get(\"aspect\"),\n",
    "#                 \"polarity\": item.get(\"polarity\"),\n",
    "#                 \"emotion\": item.get(\"emotion\")\n",
    "#             })\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "# gold_df = explode(gold)\n",
    "# pred_df = explode(pred)\n",
    "\n",
    "# print(\"Gold rows:\", len(gold_df))\n",
    "# print(\"Pred rows:\", len(pred_df))\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # MERGE GOLD + PRED ON (ID, ASPECT, POLARITY)\n",
    "# # ------------------------------------------------------------\n",
    "# merged = gold_df.merge(\n",
    "#     pred_df,\n",
    "#     on=[\"id\", \"aspect\", \"polarity\"],\n",
    "#     suffixes=(\"_gold\", \"_pred\"),\n",
    "#     how=\"outer\"\n",
    "# )\n",
    "\n",
    "# print(\"Merged rows:\", len(merged))\n",
    "\n",
    "# # Drop rows where emotion missing in gold or pred\n",
    "# valid = merged.dropna(subset=[\"emotion_gold\", \"emotion_pred\"])\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # EMOTION F1\n",
    "# # ------------------------------------------------------------\n",
    "# y_true = valid[\"emotion_gold\"]\n",
    "# y_pred = valid[\"emotion_pred\"]\n",
    "\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"EMOTION SCORES\")\n",
    "# print(\"==============================\")\n",
    "# print(\"Emotion Macro-F1:\", f1_score(y_true, y_pred, average='macro'))\n",
    "# print(\"Emotion Micro-F1:\", f1_score(y_true, y_pred, average='micro'))\n",
    "\n",
    "# print(\"\\nClassification report:\")\n",
    "# print(classification_report(y_true, y_pred))\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # ASPECT F1 (if full ABSA predictions contain aspects)\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"ASPECT SCORES\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# aspect_true = gold_df[\"aspect\"]\n",
    "# aspect_pred = pred_df[\"aspect\"]\n",
    "\n",
    "# aspect_f1 = f1_score(aspect_true, aspect_pred, average=\"macro\")\n",
    "# print(\"Aspect Macro-F1:\", aspect_f1)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # POLARITY F1\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"POLARITY SCORES\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# pol_true = gold_df[\"polarity\"]\n",
    "# pol_pred = pred_df[\"polarity\"]\n",
    "\n",
    "# pol_f1 = f1_score(pol_true, pol_pred, average=\"macro\")\n",
    "# print(\"Polarity Macro-F1:\", pol_f1)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FULL ABSA TRIPLE ACCURACY\n",
    "# # aspect + polarity + emotion must match exactly\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"FULL ABSA TRIPLE ACCURACY\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# merged[\"triple_gold\"] = list(\n",
    "#     zip(merged[\"aspect\"], merged[\"polarity\"], merged[\"emotion_gold\"])\n",
    "# )\n",
    "# merged[\"triple_pred\"] = list(\n",
    "#     zip(merged[\"aspect\"], merged[\"polarity\"], merged[\"emotion_pred\"])\n",
    "# )\n",
    "\n",
    "# merged[\"correct_triple\"] = merged[\"triple_gold\"] == merged[\"triple_pred\"]\n",
    "# triple_accuracy = merged[\"correct_triple\"].mean()\n",
    "\n",
    "# print(\"Full ABSA Accuracy:\", triple_accuracy)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # SUMMARY TABLE\n",
    "# # ------------------------------------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"SUMMARY\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# print(f\"\"\"\n",
    "# Emotion Macro-F1      : {f1_score(y_true, y_pred, average='macro'):.4f}\n",
    "# Emotion Micro-F1      : {f1_score(y_true, y_pred, average='micro'):.4f}\n",
    "# Aspect Macro-F1       : {aspect_f1:.4f}\n",
    "# Polarity Macro-F1     : {pol_f1:.4f}\n",
    "# Full ABSA Accuracy    : {triple_accuracy:.4f}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45cc69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gold rows: 300\n",
      "Loaded pred rows: 300\n",
      "\n",
      "==============================\n",
      "FINAL EVALUATION REPORT\n",
      "==============================\n",
      "\n",
      "ASPECT CLASSIFICATION\n",
      "------------------------------\n",
      "Macro F1: 0.4346\n",
      "\n",
      "POLARITY CLASSIFICATION\n",
      "------------------------------\n",
      "Macro F1: 0.3454\n",
      "\n",
      "EMOTION CLASSIFICATION\n",
      "------------------------------\n",
      "Emotion Macro-F1 : 0.2475\n",
      "Emotion Micro-F1 : 0.2965\n",
      "\n",
      "FULL ABSA TRIPLE MATCH\n",
      "------------------------------\n",
      "Full ABSA Accuracy: 0.2175\n",
      "\n",
      "==============================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "==============================\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Admiration       0.49      0.37      0.42        52\n",
      "     Annoyance       0.35      0.55      0.43        82\n",
      "  Appreciation       0.00      0.00      0.00         0\n",
      "      Approval       0.35      0.41      0.38        44\n",
      "     Confusion       0.33      0.22      0.27         9\n",
      "Disappointment       0.30      0.39      0.34        70\n",
      "   Disapproval       0.36      0.17      0.23        24\n",
      "       Disgust       0.00      0.00      0.00         0\n",
      "    Excitement       0.00      0.00      0.00         2\n",
      "          Fear       1.00      1.00      1.00         1\n",
      "   Frustration       0.29      0.32      0.30        37\n",
      "     Gratitude       0.43      0.55      0.48        11\n",
      "     Impressed       0.24      0.22      0.23        18\n",
      "   Indifferent       0.69      0.10      0.18       198\n",
      "           Joy       0.14      0.06      0.08        17\n",
      "   Realization       0.00      0.00      0.00         0\n",
      "    Relaxation       0.50      0.50      0.50         8\n",
      "        Relief       0.00      0.00      0.00         3\n",
      "  Satisfaction       0.26      0.65      0.37        26\n",
      "      Surprise       0.00      0.00      0.00         5\n",
      "          none       0.00      0.00      0.00         0\n",
      "\n",
      "      accuracy                           0.30       607\n",
      "     macro avg       0.27      0.26      0.25       607\n",
      "  weighted avg       0.45      0.30      0.29       607\n",
      "\n",
      "\n",
      "Saved â†’ eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "GOLD_PATH = os.path.join(data_root, \"cleaned_300.jsonl\")\n",
    "PRED_PATH = \"/Users/hd/Desktop/EMOTION-PRED/src/notebooks/output/gemini_annotated_aspect_polarity_emotions_200.jsonl\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load JSONL\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    return [json.loads(line) for line in open(path, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "gold = load_jsonl(GOLD_PATH)\n",
    "pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# Map gold/pred by input text\n",
    "gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Triple alignment\n",
    "# -----------------------------\n",
    "def align_triples(gold_list, pred_list):\n",
    "    \"\"\"\n",
    "    Ensures aligned triples for scoring:\n",
    "    - If pred has MORE triples â†’ truncate\n",
    "    - If pred has FEWER triples â†’ pad with ('none','none','none')\n",
    "    \"\"\"\n",
    "    g = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in gold_list]\n",
    "    p = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in pred_list]\n",
    "\n",
    "    gold_n = len(g)\n",
    "    pred_n = len(p)\n",
    "\n",
    "    # truncate hallucinations\n",
    "    if pred_n > gold_n:\n",
    "        p = p[:gold_n]\n",
    "\n",
    "    # pad missing predictions\n",
    "    if pred_n < gold_n:\n",
    "        pad = [(\"none\", \"none\", \"none\")] * (gold_n - pred_n)\n",
    "        p.extend(pad)\n",
    "\n",
    "    return g, p\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Collect aligned labels\n",
    "# -----------------------------\n",
    "all_gold_as, all_pred_as = [], []\n",
    "all_gold_pol, all_pred_pol = [], []\n",
    "all_gold_emo, all_pred_emo = [], []\n",
    "\n",
    "gold_triples_full = []\n",
    "pred_triples_full = []\n",
    "\n",
    "for text, gold_list in gold_map.items():\n",
    "\n",
    "    pred_list = pred_map.get(text, [])\n",
    "\n",
    "    g_aligned, p_aligned = align_triples(gold_list, pred_list)\n",
    "\n",
    "    for (ga, gp, ge), (pa, pp, pe) in zip(g_aligned, p_aligned):\n",
    "        all_gold_as.append(ga)\n",
    "        all_gold_pol.append(gp)\n",
    "        all_gold_emo.append(ge)\n",
    "\n",
    "        all_pred_as.append(pa)\n",
    "        all_pred_pol.append(pp)\n",
    "        all_pred_emo.append(pe)\n",
    "\n",
    "        gold_triples_full.append((ga, gp, ge))\n",
    "        pred_triples_full.append((pa, pp, pe))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Compute metrics\n",
    "# -----------------------------\n",
    "aspect_f1 = f1_score(all_gold_as, all_pred_as, average=\"macro\", zero_division=0)\n",
    "polarity_f1 = f1_score(all_gold_pol, all_pred_pol, average=\"macro\", zero_division=0)\n",
    "emotion_macro = f1_score(all_gold_emo, all_pred_emo, average=\"macro\", zero_division=0)\n",
    "emotion_micro = f1_score(all_gold_emo, all_pred_emo, average=\"micro\", zero_division=0)\n",
    "\n",
    "# Full ABSA accuracy (exact triple match)\n",
    "match = sum(1 for g, p in zip(gold_triples_full, pred_triples_full) if g == p)\n",
    "exact_acc = match / len(gold_triples_full)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Pretty PRINT\n",
    "# -----------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"FINAL EVALUATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "print(\"ASPECT CLASSIFICATION\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Macro F1: {aspect_f1:.4f}\\n\")\n",
    "\n",
    "print(\"POLARITY CLASSIFICATION\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Macro F1: {polarity_f1:.4f}\\n\")\n",
    "\n",
    "print(\"EMOTION CLASSIFICATION\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Emotion Macro-F1 : {emotion_macro:.4f}\")\n",
    "print(f\"Emotion Micro-F1 : {emotion_micro:.4f}\\n\")\n",
    "\n",
    "print(\"FULL ABSA TRIPLE MATCH\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Full ABSA Accuracy: {exact_acc:.4f}\\n\")\n",
    "\n",
    "print(\"==============================\")\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "print(classification_report(all_gold_emo, all_pred_emo, zero_division=0))\n",
    "\n",
    "\n",
    "# Save optional CSV\n",
    "df = pd.DataFrame({\n",
    "    \"Aspect F1\":      [aspect_f1],\n",
    "    \"Polarity F1\":    [polarity_f1],\n",
    "    \"Emotion F1\":     [emotion_macro],\n",
    "    \"Emotion Micro\":  [emotion_micro],\n",
    "    \"Full ABSA Acc\":  [exact_acc]\n",
    "})\n",
    "df.to_csv(\"eval_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved â†’ eval_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
