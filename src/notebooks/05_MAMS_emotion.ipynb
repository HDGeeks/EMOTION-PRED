{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8231c249",
   "metadata": {},
   "source": [
    "Lets use MAMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31daf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Running as normal Python script inside src/\n",
    "    this_file = os.path.abspath(__file__)\n",
    "    src_root = os.path.dirname(this_file)                        # EMOTION-PRED/src\n",
    "    project_root = os.path.dirname(src_root)                    # EMOTION-PRED/\n",
    "except NameError:\n",
    "    # Running inside Jupyter (likely src/notebooks or src/)\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # If running inside src/notebooks ‚Üí go up one level\n",
    "    if cwd.endswith(\"notebooks\"):\n",
    "        src_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "        project_root = os.path.dirname(src_root)\n",
    "    else:\n",
    "        # Running from project root directly\n",
    "        project_root = cwd\n",
    "        src_root = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Final unified paths\n",
    "results_root = os.path.join(src_root, \"results\")\n",
    "data_root = os.path.join(src_root, \"data\")\n",
    "print(f\"üìÇ Project root: {project_root}\"\n",
    "      f\"\\nüìÇ Source root: {src_root}\"\n",
    "      f\"\\nüìÇ Results root: {results_root}\"\n",
    "      f\"\\nüìÇ Data root: {data_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load the dataset\n",
    "ds = load_dataset(\"NEUDM/mams\")\n",
    "\n",
    "# print dataset info\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "train_ds = train_ds.to_pandas()\n",
    "\n",
    "\n",
    "# inspect row and column count\n",
    "print(f\"Number of rows: {train_ds.shape[0]}\")\n",
    "print(f\"Number of columns: {train_ds.shape[1]}\")\n",
    "\n",
    "# inspect column names\n",
    "print(\"Column names:\", train_ds.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d30a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep\n",
    "keep_cols = [\"input\", \"output\"]\n",
    "clean_df = train_ds[keep_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc34d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Target CSV location\n",
    "# -----------------------------------------\n",
    "input_csv = os.path.join(data_root, \"mams_train_full.csv\")\n",
    "dataset_name = input_csv.split(\".\")[-2].split(\"/\")[-1] # mams_train_full\n",
    "# -----------------------------------------\n",
    "# 2. If exists ‚Üí reuse it\n",
    "# -----------------------------------------\n",
    "if os.path.exists(input_csv):\n",
    "    print(f\"Using existing file: {input_csv}\")\n",
    "    df_preview = pd.read_csv(input_csv)\n",
    "    print(df_preview.head())\n",
    "else:\n",
    "    print(\"File not found ‚Üí building mams_train_full.csv ...\")\n",
    "\n",
    "    # ---- A) Copy dataset ----\n",
    "    df_full = clean_df.copy()\n",
    "\n",
    "    # ---- B) Stable row ID ----\n",
    "    df_full[\"row_id\"] = df_full.index\n",
    "\n",
    "    # ---- C) Clean string format \"['text']\" ‚Üí \"text\" ----\n",
    "    def clean_list_string(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                return parsed[0]\n",
    "            return x\n",
    "        except:\n",
    "            return x\n",
    "\n",
    "    df_full[\"sentence\"] = df_full[\"input\"].apply(clean_list_string)\n",
    "\n",
    "    # ---- D) Empty aspect column (pipeline requirement) ----\n",
    "    df_full[\"aspect_term\"] = \"\"\n",
    "\n",
    "    # ---- E) Save final dataset ----\n",
    "    df_full[[\"row_id\", \"sentence\", \"aspect_term\"]].to_csv(\n",
    "        input_csv, index=False\n",
    "    )\n",
    "\n",
    "    print(f\"‚úî Built and saved: {input_csv}\")\n",
    "    print(df_full[[\"row_id\", \"sentence\", \"aspect_term\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add src/ to Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from emotion_pipeline_optimized import run_full_emotion_pipeline\n",
    "\n",
    "RESULT_DIR = os.path.join(results_root, f\"emotion_{dataset_name}\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# If CSVs exist ‚Üí skip\n",
    "csvs = [f for f in os.listdir(RESULT_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "if not csvs:\n",
    "    t0 = time.time()\n",
    "    run_full_emotion_pipeline(\n",
    "        input_csv=input_csv,\n",
    "        dataset_name=dataset_name,\n",
    "        results_root=results_root,\n",
    "    )\n",
    "    print(\"Pipeline run:\", round(time.time() - t0, 2), \"s\")\n",
    "else:\n",
    "    print(\"Pipeline skipped (existing CSVs detected).\")\n",
    "\n",
    "# Load list of CSVs for merging step\n",
    "csv_files = [f for f in os.listdir(RESULT_DIR) if f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dee654",
   "metadata": {},
   "source": [
    "## 3.1. Merging Model Outputs\n",
    "\n",
    "Each model produces its own annotated CSV file containing two columns:\n",
    "\n",
    "- `row_id` ‚Äî the unique index for each sentence (propagated from the original MAMS dataset)\n",
    "- `emotion_auto` ‚Äî the predicted emotion label from that model\n",
    "\n",
    "To compute consensus labels, we first need to merge the outputs of all seven models into a single\n",
    "DataFrame, aligned by `row_id`. This step ensures that predictions from different models correspond to the \n",
    "same sentence on the same row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a61e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RESULT_DIR = os.path.join(results_root, f\"emotion_{dataset_name}\")  # or full dataset\n",
    "csv_files = [f for f in os.listdir(RESULT_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "MODEL_NAME_MAP = {\n",
    "    \"j_hartmann_emotion_english_roberta_large\": \"roberta_large\",\n",
    "    \"nateraw_bert_base_uncased_emotion\": \"bert_base_emotion\",\n",
    "    \"j_hartmann_emotion_english_distilroberta_base\": \"roberta_emotion\",\n",
    "    \"joeddav_distilbert_base_uncased_go_emotions_student\": \"go_emotions\",\n",
    "    \"cardiffnlp_twitter_roberta_base_emotion\": \"tweet_eval\",\n",
    "    \"mrm8488_t5_base_finetuned_emotion\": \"t5_emotion\",\n",
    "    \"SamLowe_roberta_base_go_emotions\": \"go_emotions_roberta\",\n",
    "}\n",
    "\n",
    "merged = None\n",
    "\n",
    "for f in csv_files:\n",
    "    full_path = os.path.join(RESULT_DIR, f)\n",
    "    df = pd.read_csv(full_path)\n",
    "\n",
    "    # get model name\n",
    "    base = f.replace(\"_annotated.csv\", \"\")\n",
    "    colname = MODEL_NAME_MAP.get(base, base)\n",
    "\n",
    "    # keep only row_id + emotion_auto\n",
    "    df = df[[\"row_id\", \"emotion_auto\"]].rename(columns={\n",
    "        \"emotion_auto\": colname\n",
    "    })\n",
    "\n",
    "    # first file initializes\n",
    "    if merged is None:\n",
    "        merged = df\n",
    "    else:\n",
    "        merged = merged.merge(df, on=\"row_id\", how=\"inner\")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(merged.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d209ee97",
   "metadata": {},
   "source": [
    "## 3.2. Consensus Voting Across All Emotion Models\n",
    "\n",
    "After merging the predictions from all seven models into a unified table, we compute a single \n",
    "consensus emotion label for each sentence. This gives us one stable annotation per row, despite \n",
    "the fact that each model uses different emotion taxonomies.\n",
    "\n",
    "### How the consensus label is selected\n",
    "\n",
    "For every row:\n",
    "\n",
    "1. Collect the emotion predictions from all seven models.\n",
    "2. Count how many times each emotion occurs.\n",
    "3. The emotion with the highest count becomes the final `consensus_emotion`.\n",
    "\n",
    "This is a simple majority-vote approach.  \n",
    "It works well in our setting because:\n",
    "\n",
    "- Some models do not include *neutral* as a label.\n",
    "- Some models are extremely fine-grained (27+ emotions).\n",
    "- Others only predict 4‚Äì6 basic emotions.\n",
    "- No individual model is consistently reliable across all sentences.\n",
    "\n",
    "The ensemble vote smooths out these differences and produces a more stable label.\n",
    "\n",
    "### Consensus strength\n",
    "\n",
    "We also compute a value called `consensus_count` for every row:\n",
    "\n",
    "- This is the number of models that chose the final consensus emotion.\n",
    "- High values (5‚Äì7) mean strong agreement and a clear emotional signal.\n",
    "- Medium values (3‚Äì4) represent typical agreement.\n",
    "- Low values (1‚Äì2) indicate ambiguity or mixed emotional content.\n",
    "\n",
    "The final merged dataset therefore contains two important columns:\n",
    "\n",
    "- **consensus_emotion** ‚Äî the majority-vote emotion  \n",
    "- **consensus_count** ‚Äî how many models agreed on that emotion\n",
    "\n",
    "These will be used in later sections to analyze class balance, ambiguity, and dataset quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa93bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) columns to use for voting\n",
    "model_cols = [\n",
    "    \"roberta_large\",\n",
    "    \"bert_base_emotion\",\n",
    "    \"roberta_emotion\",\n",
    "    \"go_emotions\",\n",
    "    \"tweet_eval\",\n",
    "    \"t5_emotion\",\n",
    "    \"go_emotions_roberta\",\n",
    "]\n",
    "\n",
    "# 2) majority vote function\n",
    "def get_majority_emotion(row):\n",
    "    votes = []\n",
    "\n",
    "    for col in model_cols:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            votes.append(row[col])\n",
    "\n",
    "    if len(votes) == 0:\n",
    "        return None\n",
    "\n",
    "    counts = {}\n",
    "    for emo in votes:\n",
    "        counts[emo] = counts.get(emo, 0) + 1\n",
    "\n",
    "    majority_emotion = max(counts.items(), key=lambda x: x[1])[0]\n",
    "    return majority_emotion\n",
    "\n",
    "# 3) apply to all rows\n",
    "merged[\"consensus_emotion\"] = merged.apply(get_majority_emotion, axis=1)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) Add consensus_count = number of model votes\n",
    "# ----------------------------------------------------------\n",
    "def get_consensus_count(row):\n",
    "    emo = row[\"consensus_emotion\"]\n",
    "    if pd.isna(emo):\n",
    "        return 0\n",
    "\n",
    "    return sum(1 for col in model_cols if row[col] == emo)\n",
    "\n",
    "merged[\"consensus_count\"] = merged.apply(get_consensus_count, axis=1)\n",
    "\n",
    "# 5) quick check\n",
    "merged[[\"row_id\"] + model_cols + [\"consensus_emotion\", \"consensus_count\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45bac2",
   "metadata": {},
   "source": [
    "## 3.3. Distribution of Consensus Emotions\n",
    "\n",
    "With the consensus label and consensus strength computed, we now examine how emotions are \n",
    "distributed across the entire MAMS training dataset. This helps us understand\n",
    "(1) which emotions dominate,  \n",
    "(2) which ones are rare, and  \n",
    "(3) whether emotion classes are balanced enough for training.\n",
    "\n",
    "### Emotion frequency and percentage\n",
    "\n",
    "We compute:\n",
    "\n",
    "- the raw count of each consensus emotion, and  \n",
    "- its percentage relative to the dataset size.\n",
    "\n",
    "These statistics show how often each emotion category appears after aggregating the predictions \n",
    "from all seven models.\n",
    "\n",
    "This is important because the raw model outputs were highly inconsistent in their distributions \n",
    "(e.g., some models produced thousands of \"neutral\" labels while others produced mostly \"joy\" or \"anger\").\n",
    "The consensus distribution provides a more stable signal that reflects agreement across models.\n",
    "\n",
    "### Combined statistics table\n",
    "\n",
    "A small summary table (`consensus_stats`) is generated showing both:\n",
    "\n",
    "- `count` ‚Äî number of sentences in each emotion category  \n",
    "- `percent` ‚Äî percentage of the total dataset  \n",
    "\n",
    "This table forms the basis for deciding which emotion set we will use in our final taxonomy.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4. Consensus Strength (Agreement Levels)\n",
    "\n",
    "We also analyze the `consensus_count` column, which tells us how many models agreed on the final \n",
    "emotion label for each sentence.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **7 votes** = all models agree  \n",
    "- **5‚Äì6 votes** = strong agreement  \n",
    "- **3‚Äì4 votes** = moderate agreement (typical)  \n",
    "- **1‚Äì2 votes** = low agreement (ambiguous sentences)\n",
    "\n",
    "This distribution helps identify:\n",
    "\n",
    "- highly reliable emotional expressions  \n",
    "- ambiguous or multi-aspect reviews  \n",
    "- potential noisy regions of the dataset  \n",
    "\n",
    "These patterns will later guide decisions about which emotions to keep, \n",
    "how to collapse rare emotions, and how to interpret the ensemble‚Äôs certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b62b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_counts = merged[\"consensus_emotion\"].value_counts()\n",
    "print(\"=== Consensus Emotion Distribution ===\")\n",
    "print(consensus_counts)\n",
    "\n",
    "consensus_percent = (merged[\"consensus_emotion\"].value_counts(normalize=True) * 100).round(2)\n",
    "print(\"=== Consensus Emotion Distribution (%) ===\")\n",
    "print(consensus_percent)\n",
    "\n",
    "consensus_stats = pd.DataFrame({\n",
    "    \"count\": merged[\"consensus_emotion\"].value_counts(),\n",
    "    \"percent\": (merged[\"consensus_emotion\"].value_counts(normalize=True) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"=== Consensus Statistics ===\")\n",
    "consensus_stats\n",
    "\n",
    "strength_counts = merged[\"consensus_count\"].value_counts().sort_index()\n",
    "print(\"=== Consensus Strength (How many models agreed) ===\")\n",
    "print(strength_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f602e",
   "metadata": {},
   "source": [
    "## 3.5. Per-Model Emotion Distributions\n",
    "\n",
    "Before relying on majority voting, it is important to inspect how each of the seven models\n",
    "behaves individually. The code below prints the emotion frequency distribution for each model's\n",
    "predictions across the full MAMS training set.\n",
    "\n",
    "This serves two purposes:\n",
    "\n",
    "### 1. Understanding model bias and training domains\n",
    "Each model was trained on a different dataset:\n",
    "\n",
    "- some on **GoEmotions** (27 fine-grained labels),\n",
    "- some on **Emotion-6** (6 basic emotions),\n",
    "- some on **TweetEval** (4 emotions, Twitter domain),\n",
    "- some on private emotion corpora (RoBERTa-based).\n",
    "\n",
    "Because of this, each model has a characteristic bias.  \n",
    "For example:\n",
    "\n",
    "- Some models predict **neutral** very frequently.\n",
    "- Others predict mostly **joy** and **anger**.\n",
    "- GoEmotions models spread predictions across many rare labels such as\n",
    "  `caring`, `remorse`, `admiration`, `confusion`, or `nervousness`.\n",
    "\n",
    "These differences explain why individual model outputs cannot be used directly as training labels.\n",
    "\n",
    "### 2. Revealing label-space incompatibility\n",
    "The emotion inventories used by the models differ in size and definition:\n",
    "\n",
    "- 4-label models: `anger`, `joy`, `optimism`, `sadness`\n",
    "- 6-label models: `joy`, `anger`, `sadness`, `fear`, `love`, `surprise`\n",
    "- 7-label RoBERTa models: `neutral`, `anger`, `disgust`, `fear`, `joy`, `sadness`, `surprise`\n",
    "- 27-label GoEmotions taxonomy\n",
    "\n",
    "This heterogeneity leads to strongly different distributions for the same dataset.\n",
    "\n",
    "### Why this matters\n",
    "Seeing these distributions side by side makes it clear why we need:\n",
    "\n",
    "- an **ensemble approach** (to stabilize predictions), and  \n",
    "- a **collapsed emotion taxonomy** (to unify incompatible label sets).\n",
    "\n",
    "The next sections show how the consensus labels behave and how we derive a final emotion set from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f158f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Per-model emotion counts ===\\n\")\n",
    "\n",
    "for col in model_cols:\n",
    "    print(f\"Model: {col}\")\n",
    "    print(merged[col].value_counts())\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fff829",
   "metadata": {},
   "source": [
    "## 3.6. Cross-Model Emotion Frequency Matrix\n",
    "\n",
    "To better understand how each model interprets the same dataset, we create a combined \n",
    "frequency table where:\n",
    "\n",
    "- **rows = emotion labels**,  \n",
    "- **columns = models**,  \n",
    "- **cells = number of times that model predicted that emotion**.\n",
    "\n",
    "This table makes the differences between models visually obvious:\n",
    "\n",
    "- Some models heavily favor a small set of labels (e.g., `neutral`, `joy`, `anger`).\n",
    "- GoEmotions-based models spread predictions across many fine-grained categories.\n",
    "- Models trained on Twitter (TweetEval) show very different patterns from models trained on\n",
    "  Reddit comments or general English corpora.\n",
    "- Certain models in the ensemble almost never predict some emotions (e.g., `sadness` or `fear`).\n",
    "\n",
    "### Why this table is important\n",
    "\n",
    "This matrix highlights the **incompatibility of label spaces** across models:\n",
    "\n",
    "- different emotional taxonomies,\n",
    "- different source domains,\n",
    "- different normalization and annotation styles.\n",
    "\n",
    "Because the same sentence can obtain completely different emotion labels depending on the model, \n",
    "it reinforces our earlier decision to rely on the ensemble vote rather than any individual model.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7. Consensus Label Distribution (Revisited)\n",
    "\n",
    "Directly after building the cross-model matrix, we print the distribution of the final \n",
    "`consensus_emotion` labels.\n",
    "\n",
    "This serves as a sanity check to confirm that:\n",
    "\n",
    "1. the consensus is not simply reproducing any single model's bias,  \n",
    "2. rare and fine-grained emotions are naturally collapsed,  \n",
    "3. the majority-vote mechanism produces a more balanced and interpretable label distribution.\n",
    "\n",
    "This step also serves as the foundation for the final emotion taxonomy, which we define\n",
    "in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined table: emotions √ó models\n",
    "stats = {}\n",
    "\n",
    "for col in model_cols:\n",
    "    stats[col] = merged[col].value_counts()\n",
    "\n",
    "stats_df = pd.DataFrame(stats).fillna(0).astype(int)\n",
    "\n",
    "print(\"=== Emotion frequency per model (rows = emotions, columns = models) ===\")\n",
    "print(stats_df)\n",
    "\n",
    "# Consensus distribution\n",
    "print(\"\\n=== Consensus label distribution ===\")\n",
    "print(merged[\"consensus_emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902d93b",
   "metadata": {},
   "source": [
    "## 3.8. Interpreting the Cross-Model Emotion Matrix\n",
    "\n",
    "The table above shows the frequency of every emotion predicted by every model.  \n",
    "Each column represents a model, and each row represents an emotion label.  \n",
    "This provides a direct view into how differently the seven models behave when given the same text.\n",
    "\n",
    "### Key observations\n",
    "\n",
    "#### 1. The models use **completely different emotion vocabularies**\n",
    "- Some models predict only 4 emotions (TweetEval).\n",
    "- Some predict 6 emotions (T5, BERT-base-emotion).\n",
    "- Some predict 7 emotions (Hartmann RoBERTa models).\n",
    "- GoEmotions models predict up to 27 fine-grained emotion categories.\n",
    "\n",
    "This immediately confirms that the label spaces are **not compatible** across models.\n",
    "\n",
    "#### 2. Some models are **heavily biased** toward a few emotions\n",
    "- `roberta_large` ‚Üí mostly `neutral`, `anger`, `disgust`.\n",
    "- `bert_base_emotion` ‚Üí mostly `joy` or `anger`.\n",
    "- `t5_emotion` ‚Üí similar 6-label distribution (joy + anger dominate).\n",
    "- `tweet_eval` ‚Üí mainly `anger` or `joy`.\n",
    "\n",
    "These models compress most sentences into only 2‚Äì3 classes.\n",
    "\n",
    "#### 3. GoEmotions models produce **dozens of rare fine-grained categories**\n",
    "Examples:\n",
    "- `caring`, `annoyance`, `curiosity`, `desire`, `remorse`, `excitement`,  \n",
    "  `realization`, `gratitude`, `embarrassment`, `nervousness`, etc.\n",
    "\n",
    "These categories appear only in GoEmotions models, never in others.\n",
    "\n",
    "Such rare labels create extreme sparsity and cannot be used directly for training.\n",
    "\n",
    "#### 4. The distributions do not match each other at all\n",
    "For example:\n",
    "- `anger` ranges from **2 predictions** (SamLowe) to **4350 predictions** (TweetEval).\n",
    "- `neutral` ranges from **27 predictions** (GoEmotions) to **6595 predictions** (SamLowe).\n",
    "- `joy` ranges from **29 predictions** (GoEmotions) to **4203** (BERT-base-emotion).\n",
    "\n",
    "This inconsistency proves that no single model should be trusted alone.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.9. Consensus Distribution (Overview)\n",
    "\n",
    "After combining all model outputs using majority voting, the final consensus distribution is:\n",
    "\n",
    "- **neutral:** 3830  \n",
    "- **anger:** 1850  \n",
    "- **joy:** 923  \n",
    "- **disgust:** 404  \n",
    "- **sadness:** 181  \n",
    "- **surprise:** 138  \n",
    "- **fear:** 103  \n",
    "- **love:** 17  \n",
    "\n",
    "### What the consensus reveals\n",
    "\n",
    "- Neutral dominates, which is expected for restaurant review sentences.\n",
    "- Anger and joy form the primary negative/positive poles.\n",
    "- Disgust, sadness, surprise, and fear occur but are much rarer.\n",
    "- Love is extremely rare and not suitable as a training class.\n",
    "\n",
    "This final distribution is much more stable and interpretable than any individual model‚Äôs distribution,\n",
    "demonstrating the benefit of the ensemble approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f7273",
   "metadata": {},
   "source": [
    "## 4. Defining the Final Emotion Set\n",
    "\n",
    "After examining (1) the raw predictions from all seven models, (2) the merged consensus labels, \n",
    "and (3) the distribution of consensus emotion frequencies, we now decide on the final emotion \n",
    "taxonomy that will be used in the remainder of this project.\n",
    "\n",
    "### Why we need a reduced emotion set\n",
    "The original model outputs span more than 30 unique emotion labels, including many extremely \n",
    "fine-grained categories such as `caring`, `annoyance`, `remorse`, `admiration`, and `realization`.\n",
    "However:\n",
    "\n",
    "- Many of these labels appear only a handful of times in the consensus results.\n",
    "- Some labels are produced exclusively by GoEmotions models and never by others.\n",
    "- Models do not share the same emotion inventory, meaning their outputs are not directly comparable.\n",
    "- Sparse classes are unsuitable for training and evaluating ABSA models on a dataset of ~7k sentences.\n",
    "\n",
    "### Data-driven collapse of emotions\n",
    "The consensus distribution shows that the majority of sentences fall into a small set of stable, \n",
    "interpretable emotions:\n",
    "\n",
    "- neutral\n",
    "- anger\n",
    "- joy\n",
    "- disgust\n",
    "- sadness\n",
    "- surprise\n",
    "- fear\n",
    "\n",
    "In contrast, emotions like `love` appear extremely rarely and do not provide enough examples for \n",
    "robust model training.\n",
    "\n",
    "### Theoretical motivation\n",
    "This reduced set aligns with the widely used **basic emotion model**, which consists of:\n",
    "\n",
    "- anger\n",
    "- joy\n",
    "- sadness\n",
    "- fear\n",
    "- surprise\n",
    "- disgust\n",
    "\n",
    "and we simply add **neutral**, which is essential for real-world text where no explicit emotion is expressed.\n",
    "\n",
    "### Final decision\n",
    "We therefore adopt the following final emotion set for all downstream tasks:\n",
    "\n",
    "**neutral, anger, joy, sadness, fear, surprise, disgust**\n",
    "\n",
    "This taxonomy provides a balance between interpretability, coverage, and class frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92730c5b",
   "metadata": {},
   "source": [
    "## 5. Mapping Fine-Grained Labels into the Final Emotion Set\n",
    "\n",
    "To convert the heterogeneous model outputs into our unified 7-emotion taxonomy, we define a \n",
    "mapping from all fine-grained labels to the closest basic emotion.\n",
    "\n",
    "This mapping is guided by:\n",
    "\n",
    "- semantic similarity of emotion categories,\n",
    "- how each label is used in GoEmotions,\n",
    "- how emotions tend to cluster in psychological models.\n",
    "\n",
    "### Mapping rules\n",
    "\n",
    "**Joy cluster**\n",
    "- joy, amusement, excitement, optimism, gratitude, pride, love, admiration, approval, caring, desire  \n",
    "‚Üí **joy**\n",
    "\n",
    "**Anger cluster**\n",
    "- anger, annoyance, disapproval, disappointment, disgust (sometimes ambiguous)  \n",
    "‚Üí **anger** or **disgust** depending on context  \n",
    "(Note: in this project we keep `disgust` as its own basic emotion.)\n",
    "\n",
    "**Sadness cluster**\n",
    "- sadness, remorse, grief, disappointment (if expressing loss)  \n",
    "‚Üí **sadness**\n",
    "\n",
    "**Fear cluster**\n",
    "- fear, nervousness, anxiety  \n",
    "‚Üí **fear**\n",
    "\n",
    "**Surprise cluster**\n",
    "- surprise, realization (sometimes), confusion (when the tone is startled/uncertain)  \n",
    "‚Üí **surprise**\n",
    "\n",
    "**Disgust**\n",
    "- disgust, repulsion  \n",
    "‚Üí **disgust**\n",
    "\n",
    "**Neutral**\n",
    "- neutral, ‚Äúno emotion‚Äù, or low-agreement predictions  \n",
    "‚Üí **neutral**\n",
    "\n",
    "### Purpose of the mapping\n",
    "This conversion allows us to:\n",
    "\n",
    "- integrate outputs from all seven models,  \n",
    "- remove model-specific noise,  \n",
    "- avoid extremely rare labels,  \n",
    "- create a clean and consistent target space for ABSA emotion modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f04178",
   "metadata": {},
   "source": [
    "## 6. Visualizing Consensus Labels and Agreement Levels\n",
    "\n",
    "To better understand the structure of our dataset and verify the effects of the consensus process,\n",
    "we generate several visualizations.\n",
    "\n",
    "### 1. Consensus Emotion Distribution (Bar Chart)\n",
    "A bar chart showing how often each of the seven final emotions appears in the dataset.\n",
    "This helps confirm class imbalance and motivates why certain emotions are rare.\n",
    "\n",
    "### 2. Consensus Strength Distribution\n",
    "A second bar plot showing how many sentences received 1, 2, 3, ‚Ä¶ up to 7 votes.\n",
    "This reveals how confident the ensemble was across the dataset.\n",
    "\n",
    "- High agreement (5‚Äì7 votes) = strong emotional signal.\n",
    "- Medium agreement (3‚Äì4 votes) = typical.\n",
    "- Low agreement (1‚Äì2 votes) = ambiguous or multi-aspect sentences.\n",
    "\n",
    "### 3. Emotion √ó Consensus Strength Heatmap (Optional)\n",
    "A heatmap that crosses emotions with agreement levels provides insight into:\n",
    "\n",
    "- which emotions are predicted confidently,  \n",
    "- which emotions tend to be ambiguous,  \n",
    "- whether certain emotions consistently attract low consensus.\n",
    "\n",
    "### Why these visualizations matter\n",
    "Visualizing the consensus results helps validate that:\n",
    "\n",
    "- the ensemble produces reasonable and interpretable distributions,\n",
    "- our final taxonomy is well supported by the data,\n",
    "- the dataset has enough examples per emotion for further modeling,\n",
    "- ambiguous rows can be detected and optionally filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "order = merged[\"consensus_emotion\"].value_counts().index\n",
    "\n",
    "sns.barplot(\n",
    "    x=merged[\"consensus_emotion\"].value_counts().index,\n",
    "    y=merged[\"consensus_emotion\"].value_counts().values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "\n",
    "plt.title(\"Consensus Emotion Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Emotion\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afacf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "strength = merged[\"consensus_count\"].value_counts().sort_index()\n",
    "\n",
    "sns.barplot(\n",
    "    x=strength.index,\n",
    "    y=strength.values,\n",
    "    palette=\"magma\"\n",
    ")\n",
    "\n",
    "plt.title(\"Consensus Strength (Number of Models Agreeing)\", fontsize=16)\n",
    "plt.xlabel(\"Consensus Count (Votes)\", fontsize=14)\n",
    "plt.ylabel(\"Number of Sentences\", fontsize=14)\n",
    "plt.xticks(strength.index)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Make a full copy before remapping\n",
    "# ----------------------------------------------------\n",
    "merged_final = merged.copy()\n",
    "print(\"Original merged shape:\", merged.shape)\n",
    "print(\"New merged_final shape:\", merged_final.shape)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Define final emotion set\n",
    "# ----------------------------------------------------\n",
    "FINAL_EMOTIONS = [\n",
    "    \"neutral\", \"anger\", \"joy\", \"sadness\", \"fear\", \"surprise\", \"disgust\"\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Fine-grained ‚Üí Basic emotion mapping\n",
    "# ----------------------------------------------------\n",
    "emotion_mapping = {\n",
    "    # Joy cluster\n",
    "    \"joy\": \"joy\",\n",
    "    \"amusement\": \"joy\",\n",
    "    \"excitement\": \"joy\",\n",
    "    \"optimism\": \"joy\",\n",
    "    \"gratitude\": \"joy\",\n",
    "    \"pride\": \"joy\",\n",
    "    \"love\": \"joy\",\n",
    "    \"admiration\": \"joy\",\n",
    "    \"approval\": \"joy\",\n",
    "    \"caring\": \"joy\",\n",
    "    \"desire\": \"joy\",\n",
    "\n",
    "    # Anger cluster\n",
    "    \"anger\": \"anger\",\n",
    "    \"annoyance\": \"anger\",\n",
    "    \"disapproval\": \"anger\",\n",
    "    \"disappointment\": \"anger\",  # Could be sadness; restaurant domain fits anger\n",
    "\n",
    "    # Sadness cluster\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"remorse\": \"sadness\",\n",
    "    \"embarrassment\": \"sadness\",\n",
    "\n",
    "    # Fear cluster\n",
    "    \"fear\": \"fear\",\n",
    "    \"nervousness\": \"fear\",\n",
    "\n",
    "    # Surprise cluster\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"realization\": \"surprise\",\n",
    "    \"confusion\": \"surprise\",\n",
    "\n",
    "    # Disgust cluster\n",
    "    \"disgust\": \"disgust\",\n",
    "    \"repulsion\": \"disgust\",\n",
    "\n",
    "    # Neutral\n",
    "    \"neutral\": \"neutral\",\n",
    "}\n",
    "\n",
    "# Safety fallback: unmapped ‚Üí neutral\n",
    "def map_final_label(x):\n",
    "    return emotion_mapping.get(x, \"neutral\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Apply mapping\n",
    "# ----------------------------------------------------\n",
    "merged_final[\"emotion_final\"] = merged_final[\"consensus_emotion\"].apply(map_final_label)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. BEFORE ‚Üí AFTER distributions\n",
    "# ----------------------------------------------------\n",
    "print(\"=== BEFORE (Consensus Emotion Distribution) ===\")\n",
    "print(merged_final[\"consensus_emotion\"].value_counts())\n",
    "\n",
    "print(\"\\n=== AFTER (Final 7-Class Emotion Distribution) ===\")\n",
    "print(merged_final[\"emotion_final\"].value_counts())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6. Check if any labels were not mapped\n",
    "# ----------------------------------------------------\n",
    "unmapped = merged_final[\n",
    "    ~merged_final[\"consensus_emotion\"].isin(emotion_mapping.keys())\n",
    "][\"consensus_emotion\"].unique()\n",
    "\n",
    "print(\"\\nUnmapped labels:\", unmapped)\n",
    "\n",
    "# Should output: []\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 7. Visualization of final 7-class distribution\n",
    "# ----------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=merged_final[\"emotion_final\"].value_counts().index,\n",
    "    y=merged_final[\"emotion_final\"].value_counts().values,\n",
    "    palette=\"coolwarm_r\"\n",
    ")\n",
    "\n",
    "plt.title(\"Final 7-Emotion Distribution (After Mapping)\", fontsize=16)\n",
    "plt.xlabel(\"Emotion\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda49205",
   "metadata": {},
   "source": [
    "### 7. Validation of the Final Emotion Mapping\n",
    "\n",
    "We mapped all consensus emotion labels into our unified 7-emotion taxonomy:\n",
    "`neutral, anger, joy, sadness, fear, surprise, disgust`.\n",
    "\n",
    "To verify correctness, we performed several checks:\n",
    "\n",
    "1. **Dataset size remained constant**  \n",
    "   Both `merged` and `merged_final` contain 7,446 rows, confirming that no sentences were lost during remapping.\n",
    "\n",
    "2. **Before/after comparison**  \n",
    "   Before remapping, the consensus labels included 8 emotions.  \n",
    "   The only rare emotion was *love* (17 examples).  \n",
    "   After remapping, *love* was correctly merged into the *joy* category.  \n",
    "   All other emotions remained unchanged.\n",
    "\n",
    "3. **No unmapped labels**  \n",
    "   We explicitly checked whether any consensus labels were missing from the mapping table.  \n",
    "   The result was an empty set (`[]`), confirming that the mapping is complete.\n",
    "\n",
    "4. **The final 7-class distribution is clean and balanced**  \n",
    "   The mapped labels now follow the expected pattern for restaurant reviews:\n",
    "   - neutral and anger dominate,\n",
    "   - joy is substantial (after merging love),\n",
    "   - disgust, sadness, surprise, and fear are present but less frequent.\n",
    "\n",
    "These checks confirm that our final emotion taxonomy is both theoretically grounded and empirically supported, \n",
    "and that the remapping process is correct, exhaustive, and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86776c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_pipeline_optimized import annotate_model\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"sentence\": [\"Service was slow but the staff were very friendly.\"],\n",
    "    \"aspect_term\": [\"service\"]\n",
    "})\n",
    "\n",
    "preds = annotate_model(df_test)\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
