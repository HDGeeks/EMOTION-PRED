{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8231c249",
   "metadata": {},
   "source": [
    "Lets use MAMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31daf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Running as normal Python script inside src/\n",
    "    this_file = os.path.abspath(__file__)\n",
    "    src_root = os.path.dirname(this_file)                        # EMOTION-PRED/src\n",
    "    project_root = os.path.dirname(src_root)                    # EMOTION-PRED/\n",
    "except NameError:\n",
    "    # Running inside Jupyter (likely src/notebooks or src/)\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # If running inside src/notebooks â†’ go up one level\n",
    "    if cwd.endswith(\"notebooks\"):\n",
    "        src_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "        project_root = os.path.dirname(src_root)\n",
    "    else:\n",
    "        # Running from project root directly\n",
    "        project_root = cwd\n",
    "        src_root = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Final unified paths\n",
    "results_root = os.path.join(src_root, \"results\")\n",
    "data_root = os.path.join(src_root, \"data\")\n",
    "print(f\"ðŸ“‚ Project root: {project_root}\"\n",
    "      f\"\\nðŸ“‚ Source root: {src_root}\"\n",
    "      f\"\\nðŸ“‚ Results root: {results_root}\"\n",
    "      f\"\\nðŸ“‚ Data root: {data_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load the dataset\n",
    "ds = load_dataset(\"NEUDM/mams\")\n",
    "\n",
    "# print dataset info\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "train_ds = train_ds.to_pandas()\n",
    "\n",
    "\n",
    "# inspect row and column count\n",
    "print(f\"Number of rows: {train_ds.shape[0]}\")\n",
    "print(f\"Number of columns: {train_ds.shape[1]}\")\n",
    "\n",
    "# inspect column names\n",
    "print(\"Column names:\", train_ds.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d30a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep\n",
    "keep_cols = [\"input\", \"output\"]\n",
    "clean_df = train_ds[keep_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc34d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Take 50 rows\n",
    "df_head = clean_df[\"input\"].head(50).to_frame()\n",
    "\n",
    "# 1) Convert \"['text']\" â†’ \"text\"\n",
    "def clean_list_string(x):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)   # safely parse list-string\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            return parsed[0]\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "df_head[\"sentence\"] = df_head[\"input\"].apply(clean_list_string)\n",
    "\n",
    "# 2) Add empty aspect column (required by pipeline)\n",
    "df_head[\"aspect_term\"] = \"\"\n",
    "\n",
    "# 3) Save in the correct 2-column format\n",
    "input_csv = os.path.join(data_root, \"temp_50.csv\")\n",
    "df_head[[\"sentence\", \"aspect_term\"]].to_csv(input_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add src/ to Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_pipeline import run_full_emotion_pipeline\n",
    "\n",
    "run_full_emotion_pipeline(\n",
    "    input_csv=input_csv,\n",
    "    dataset_name=\"sample50\",\n",
    "    results_root=results_root,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a61e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RESULT_DIR = os.path.join(results_root, \"emotion_sample50\")\n",
    "\n",
    "csv_files = [f for f in os.listdir(RESULT_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "MODEL_NAME_MAP = {\n",
    "    \"j_hartmann_emotion_english_roberta_large\": \"roberta_large\",\n",
    "    \"nateraw_bert_base_uncased_emotion\": \"bert_base_emotion\",\n",
    "    \"j_hartmann_emotion_english_distilroberta_base\": \"roberta_emotion\",\n",
    "    \"joeddav_distilbert_base_uncased_go_emotions_student\": \"go_emotions\",\n",
    "    \"cardiffnlp_twitter_roberta_base_emotion\": \"tweet_eval\",\n",
    "    \"mrm8488_t5_base_finetuned_emotion\": \"t5_emotion\"\n",
    "}\n",
    "\n",
    "merged = pd.DataFrame()\n",
    "\n",
    "for f in csv_files:\n",
    "    full_path = os.path.join(RESULT_DIR, f)\n",
    "    df = pd.read_csv(full_path)\n",
    "\n",
    "    base = f.replace(\"_annotated.csv\", \"\")\n",
    "    colname = MODEL_NAME_MAP.get(base, base)   # fallback = raw name if missing\n",
    "\n",
    "    merged[colname] = df[\"emotion_auto\"]\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(merged.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
