{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e54ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Project root: /Users/hd/Desktop/EMOTION-PRED\n",
      "ðŸ“‚ Source root: /Users/hd/Desktop/EMOTION-PRED/src\n",
      "ðŸ“‚ Results root: /Users/hd/Desktop/EMOTION-PRED/src/results\n",
      "ðŸ“‚ Data root: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n",
      "Using dataset directory: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # Running as normal Python script inside src/\n",
    "    this_file = os.path.abspath(__file__)\n",
    "    src_root = os.path.dirname(this_file)                        # EMOTION-PRED/src\n",
    "    project_root = os.path.dirname(src_root)                    # EMOTION-PRED/\n",
    "except NameError:\n",
    "    # Running inside Jupyter (likely src/notebooks or src/)\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # If running inside src/notebooks â†’ go up one level\n",
    "    if cwd.endswith(\"notebooks\"):\n",
    "        src_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "        project_root = os.path.dirname(src_root)\n",
    "    else:\n",
    "        # Running from project root directly\n",
    "        project_root = cwd\n",
    "        src_root = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Final unified paths\n",
    "results_root = os.path.join(src_root, \"results\")\n",
    "data_root = os.path.join(src_root, \"data\",\"MAMS-ACSA\",\"raw\",\"data_jsonl\")\n",
    "print(f\"ðŸ“‚ Project root: {project_root}\"\n",
    "      f\"\\nðŸ“‚ Source root: {src_root}\"\n",
    "      f\"\\nðŸ“‚ Results root: {results_root}\"\n",
    "      f\"\\nðŸ“‚ Data root: {data_root}\")\n",
    "# 3 â€” JSONL files\n",
    "TRAIN_JSONL = os.path.join(data_root, \"train.jsonl\")\n",
    "VAL_JSONL   = os.path.join(data_root, \"val.jsonl\")\n",
    "TEST_JSONL  = os.path.join(data_root, \"test.jsonl\")\n",
    "SAMPLE_JSONL = os.path.join(data_root, \"sample.jsonl\")\n",
    "print(\"Using dataset directory:\", data_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c4b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: data/MAMS-ACSA/raw/data_jsonl/train.jsonl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 data/MAMS-ACSA/raw/data_jsonl/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d235581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sentence count: 296\n",
      "Saved to: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl/sample.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Load dataset\n",
    "# ------------------------------------------------------------\n",
    "PATH = TRAIN_JSONL\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Keep original index for each row (sentence)\n",
    "df[\"orig_id\"] = df.index\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Explode: each aspect becomes one row\n",
    "# ------------------------------------------------------------\n",
    "df_ex = df.explode(\"output\").reset_index(drop=True)\n",
    "\n",
    "df_ex[\"aspect\"] = df_ex[\"output\"].apply(lambda x: x[\"aspect\"])\n",
    "df_ex[\"polarity\"] = df_ex[\"output\"].apply(lambda x: x[\"polarity\"])\n",
    "\n",
    "# stratum = aspect + polarity\n",
    "df_ex[\"stratum\"] = df_ex[\"aspect\"] + \"_\" + df_ex[\"polarity\"]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Compute proportional sample sizes for 300\n",
    "# ------------------------------------------------------------\n",
    "TARGET = 300\n",
    "\n",
    "stratum_counts = df_ex[\"stratum\"].value_counts()\n",
    "weights = stratum_counts / stratum_counts.sum()\n",
    "\n",
    "sizes = (weights * TARGET).round().astype(int)\n",
    "sizes[sizes == 0] = 1   # ensure no stratum is missing\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Sample USING ORIGINAL SENTENCE IDS\n",
    "# ------------------------------------------------------------\n",
    "selected_sentence_ids = set()\n",
    "np.random.seed(42)\n",
    "\n",
    "for stratum, size in sizes.items():\n",
    "    subset = df_ex[df_ex[\"stratum\"] == stratum]\n",
    "\n",
    "    # take original sentence IDs, NOT exploded index\n",
    "    sentence_ids = subset[\"orig_id\"].unique()\n",
    "\n",
    "    n_pick = min(size, len(sentence_ids))\n",
    "    chosen = np.random.choice(sentence_ids, n_pick, replace=False)\n",
    "\n",
    "    selected_sentence_ids.update(chosen)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Extract unique sentences\n",
    "# ------------------------------------------------------------\n",
    "selected_df = df[df[\"orig_id\"].isin(selected_sentence_ids)].copy()\n",
    "selected_df = selected_df.drop(columns=[\"orig_id\"])\n",
    "selected_df = selected_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Final sentence count:\", len(selected_df))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Save as JSONL\n",
    "# ------------------------------------------------------------\n",
    "OUT_PATH = SAMPLE_JSONL\n",
    "selected_df.to_json(OUT_PATH, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Saved to:\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99d8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== ASPECT CATEGORY ======================\n",
      "               Train %  Sample %  Difference\n",
      "aspect                                      \n",
      "ambience          4.57      4.49       -0.08\n",
      "food             32.54     31.11       -1.43\n",
      "menu              6.70      7.53        0.83\n",
      "miscellaneous    13.46     14.18        0.72\n",
      "place             9.79      9.99        0.20\n",
      "price             4.54      5.21        0.67\n",
      "service           8.90      8.68       -0.22\n",
      "staff            19.51     18.81       -0.70\n",
      "\n",
      "====================== POLARITY ======================\n",
      "          Train %  Sample %  Difference\n",
      "polarity                               \n",
      "neutral     43.40     43.42        0.02\n",
      "negative    29.39     29.96        0.57\n",
      "positive    27.21     26.63       -0.58\n",
      "\n",
      "====================== ASPECT Ã— POLARITY (JOINT) ======================\n",
      "                        Train %  Sample %  Difference\n",
      "joint                                                \n",
      "ambience_negative          1.27      1.16       -0.11\n",
      "ambience_neutral           0.75      1.30        0.55\n",
      "ambience_positive          2.55      2.03       -0.52\n",
      "food_negative              3.60      3.33       -0.27\n",
      "food_neutral              18.31     16.79       -1.52\n",
      "food_positive             10.63     11.00        0.37\n",
      "menu_negative              0.55      1.30        0.75\n",
      "menu_neutral               5.25      5.07       -0.18\n",
      "menu_positive              0.90      1.16        0.26\n",
      "miscellaneous_negative     2.76      2.60       -0.16\n",
      "miscellaneous_neutral      7.49      8.54        1.05\n",
      "miscellaneous_positive     3.20      3.04       -0.16\n",
      "place_negative             1.96      2.46        0.50\n",
      "place_neutral              6.06      6.08        0.02\n",
      "place_positive             1.76      1.45       -0.31\n",
      "price_negative             1.61      1.59       -0.02\n",
      "price_neutral              1.92      2.17        0.25\n",
      "price_positive             1.02      1.45        0.43\n",
      "service_negative           4.64      4.05       -0.59\n",
      "service_neutral            1.81      2.03        0.22\n",
      "service_positive           2.45      2.60        0.15\n",
      "staff_negative            13.00     13.46        0.46\n",
      "staff_neutral              1.82      1.45       -0.37\n",
      "staff_positive             4.68      3.91       -0.77\n",
      "\n",
      "====================== KL DIVERGENCE ======================\n",
      "Aspect KL: 0.0017\n",
      "Polarity KL: 0.0\n",
      "Joint KL: 0.0073\n",
      "\n",
      "========== Chi-square test: Aspect ==========\n",
      "Chi-square: 2.3294 | p-value: 0.9394\n",
      "\n",
      "========== Chi-square test: Polarity ==========\n",
      "Chi-square: 0.1598 | p-value: 0.9232\n",
      "\n",
      "========== Chi-square test: Joint ==========\n",
      "Chi-square: 18.6096 | p-value: 0.7236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(18.60955834809744), np.float64(0.7236388294410663))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Load FULL TRAIN and SAMPLED 300\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "TRAIN_PATH = TRAIN_JSONL\n",
    "SAMPLE_PATH = SAMPLE_JSONL\n",
    "\n",
    "# Load full dataset\n",
    "train_rows = []\n",
    "with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        train_rows.append(json.loads(line))\n",
    "df_train = pd.DataFrame(train_rows)\n",
    "\n",
    "# Load sampled dataset\n",
    "sample_rows = []\n",
    "with open(SAMPLE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        sample_rows.append(json.loads(line))\n",
    "df_sample = pd.DataFrame(sample_rows)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Explode both (aspect-level rows)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def explode_df(df):\n",
    "    ex = df.explode(\"output\").reset_index(drop=True)\n",
    "    ex[\"aspect\"] = ex[\"output\"].apply(lambda x: x[\"aspect\"])\n",
    "    ex[\"polarity\"] = ex[\"output\"].apply(lambda x: x[\"polarity\"])\n",
    "    ex[\"joint\"] = ex[\"aspect\"] + \"_\" + ex[\"polarity\"]\n",
    "    return ex\n",
    "\n",
    "df_train_ex = explode_df(df_train)\n",
    "df_sample_ex = explode_df(df_sample)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Compute distributions\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def compute_dist(df, column):\n",
    "    counts = df[column].value_counts()\n",
    "    pct = (counts / counts.sum() * 100).round(2)\n",
    "    return counts, pct\n",
    "\n",
    "# Aspect-level\n",
    "train_aspect_counts, train_aspect_pct = compute_dist(df_train_ex, \"aspect\")\n",
    "sample_aspect_counts, sample_aspect_pct = compute_dist(df_sample_ex, \"aspect\")\n",
    "\n",
    "# Polarity-level\n",
    "train_pol_counts, train_pol_pct = compute_dist(df_train_ex, \"polarity\")\n",
    "sample_pol_counts, sample_pol_pct = compute_dist(df_sample_ex, \"polarity\")\n",
    "\n",
    "# Joint aspect Ã— polarity\n",
    "train_joint_counts, train_joint_pct = compute_dist(df_train_ex, \"joint\")\n",
    "sample_joint_counts, sample_joint_pct = compute_dist(df_sample_ex, \"joint\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Create comparison tables\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def compare_dists(train_pct, sample_pct, name):\n",
    "    print(f\"\\n====================== {name.upper()} ======================\")\n",
    "    df_compare = pd.DataFrame({\n",
    "        \"Train %\": train_pct,\n",
    "        \"Sample %\": sample_pct\n",
    "    }).fillna(0)\n",
    "    df_compare[\"Difference\"] = (df_compare[\"Sample %\"] - df_compare[\"Train %\"]).round(2)\n",
    "    print(df_compare)\n",
    "    return df_compare\n",
    "\n",
    "aspect_compare = compare_dists(train_aspect_pct, sample_aspect_pct, \"Aspect Category\")\n",
    "polarity_compare = compare_dists(train_pol_pct, sample_pol_pct, \"Polarity\")\n",
    "joint_compare = compare_dists(train_joint_pct, sample_joint_pct, \"Aspect Ã— Polarity (Joint)\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) KL Divergence (measure of distribution similarity)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    # Replace 0s to avoid log problems\n",
    "    p = np.array(p.replace(0, 1e-9))\n",
    "    q = np.array(q.replace(0, 1e-9))\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "kl_aspect = kl_divergence(train_aspect_pct / 100, sample_aspect_pct / 100)\n",
    "kl_polarity = kl_divergence(train_pol_pct / 100, sample_pol_pct / 100)\n",
    "kl_joint = kl_divergence(train_joint_pct / 100, sample_joint_pct / 100)\n",
    "\n",
    "print(\"\\n====================== KL DIVERGENCE ======================\")\n",
    "print(\"Aspect KL:\", round(kl_aspect, 4))\n",
    "print(\"Polarity KL:\", round(kl_polarity, 4))\n",
    "print(\"Joint KL:\", round(kl_joint, 4))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) Chi-square test (optional statistical test)\n",
    "# -------------------------------------------------------------------\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "def chi_test(train_counts, sample_counts, name):\n",
    "    # align indexes\n",
    "    idx = sorted(set(train_counts.index) | set(sample_counts.index))\n",
    "    train = np.array([train_counts.get(i,0) for i in idx])\n",
    "    sample = np.array([sample_counts.get(i,0) for i in idx])\n",
    "\n",
    "    # scale train distribution to 300 samples\n",
    "    train_scaled = train / train.sum() * sample.sum()\n",
    "\n",
    "    chi, p = chisquare(f_obs=sample, f_exp=train_scaled)\n",
    "    print(f\"\\n========== Chi-square test: {name} ==========\")\n",
    "    print(\"Chi-square:\", round(chi, 4), \"| p-value:\", round(p, 4))\n",
    "    return chi, p\n",
    "\n",
    "chi_test(train_aspect_counts, sample_aspect_counts, \"Aspect\")\n",
    "chi_test(train_pol_counts, sample_pol_counts, \"Polarity\")\n",
    "chi_test(train_joint_counts, sample_joint_counts, \"Joint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
